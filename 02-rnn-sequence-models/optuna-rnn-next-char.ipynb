{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0ac2c8",
   "metadata": {},
   "source": [
    "## Next Character Prediction - Hyperparameter Optimization with Optuna\n",
    "\n",
    "Para encontrar la mejor configuración de hiperparámetros que maximize la precisión de la predicción de caracteres, se ha llevado a cabo una optimización Bayesiana utilizando Optuna.\n",
    "\n",
    "En vez de utilizar Search Grid u otros métodos de búsqueda, se ha optado por utilizar el sampler de Optuna, que es el TPE sampler.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0523aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GPU cache limpiado\n",
      "Loading text from ./timemachine.txt...\n",
      "Total text length: 214451\n",
      "Train text length: 64335\n",
      "Validation text length: 21445\n",
      "Test text length: 128671\n",
      "Vocabulary size: 29\n",
      "Characters: [' ', '<', '>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "Creating sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-07 20:12:22,047] A new study created in memory with name: no-name-9a13da96-183a-45a5-97cf-675765ce6128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences: torch.Size([64314, 20])\n",
      "Validation sequences: torch.Size([21424, 20])\n",
      "Test sequences: torch.Size([128650, 20])\n",
      "\n",
      "================================================================================\n",
      "STARTING OPTUNA HYPERPARAMETER OPTIMIZATION\n",
      "================================================================================\n",
      "This may take a while depending on n_trials...\n",
      "\n",
      "Running 100 optimization trials...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0580453bcf2476f993396e2b4b23591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-07 20:13:16,307] Trial 0 finished with value: 0.5719753503799438 and parameters: {'hidden_size': 288, 'learning_rate': 0.00261616087122516, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.27235680375782684, 'clip_value': 4.481977404539436, 'weight_init': 'kaiming', 'patience': 7, 'batch_size': 128, 'num_epochs': 122}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:13:36,743] Trial 1 finished with value: 0.5656273365020752 and parameters: {'hidden_size': 256, 'learning_rate': 0.0024604316486968986, 'optimizer': 'rmsprop', 'activation': 'relu', 'dropout_rate': 0.1438956102906751, 'clip_value': 3.4740190797507804, 'weight_init': 'orthogonal', 'patience': 6, 'batch_size': 128, 'num_epochs': 145}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:14:06,654] Trial 2 finished with value: 0.5603995323181152 and parameters: {'hidden_size': 256, 'learning_rate': 0.0025531054135347884, 'optimizer': 'rmsprop', 'activation': 'relu', 'dropout_rate': 0.1628095941120054, 'clip_value': 4.245115878640706, 'weight_init': 'orthogonal', 'patience': 20, 'batch_size': 128, 'num_epochs': 152}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:14:54,801] Trial 3 finished with value: 0.5391616821289062 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018832347358267033, 'optimizer': 'rmsprop', 'activation': 'tanh', 'dropout_rate': 0.3451960029286174, 'clip_value': 2.6551172988066645, 'weight_init': 'normal', 'patience': 6, 'batch_size': 64, 'num_epochs': 96}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:16:06,622] Trial 4 finished with value: 0.5456497669219971 and parameters: {'hidden_size': 288, 'learning_rate': 0.002582783022579109, 'optimizer': 'adamw', 'activation': 'tanh', 'dropout_rate': 0.35619309628018997, 'clip_value': 4.0411502595033015, 'weight_init': 'xavier', 'patience': 16, 'batch_size': 128, 'num_epochs': 118}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:17:08,141] Trial 5 finished with value: 0.5486370325088501 and parameters: {'hidden_size': 288, 'learning_rate': 0.002654921697823203, 'optimizer': 'adamw', 'activation': 'tanh', 'dropout_rate': 0.08813412055811046, 'clip_value': 1.3610354203651833, 'weight_init': 'kaiming', 'patience': 19, 'batch_size': 128, 'num_epochs': 141}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:17:31,980] Trial 6 finished with value: 0.5684279203414917 and parameters: {'hidden_size': 256, 'learning_rate': 0.0020871492330956022, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3371750646077166, 'clip_value': 1.7701643062073864, 'weight_init': 'normal', 'patience': 10, 'batch_size': 128, 'num_epochs': 114}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:18:43,925] Trial 7 finished with value: 0.5645071268081665 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018064037517232033, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.38173110525200615, 'clip_value': 2.4806552465079275, 'weight_init': 'normal', 'patience': 20, 'batch_size': 128, 'num_epochs': 104}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:19:22,239] Trial 8 finished with value: 0.532160222530365 and parameters: {'hidden_size': 256, 'learning_rate': 0.0024575569482918843, 'optimizer': 'adam', 'activation': 'tanh', 'dropout_rate': 0.1263663590691914, 'clip_value': 3.345154353443328, 'weight_init': 'xavier', 'patience': 8, 'batch_size': 64, 'num_epochs': 131}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:19:47,343] Trial 9 finished with value: 0.5638536214828491 and parameters: {'hidden_size': 256, 'learning_rate': 0.0018849645097612196, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.2968205957895304, 'clip_value': 0.886256710424852, 'weight_init': 'orthogonal', 'patience': 16, 'batch_size': 128, 'num_epochs': 91}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:20:52,435] Trial 10 finished with value: 0.5575056076049805 and parameters: {'hidden_size': 288, 'learning_rate': 0.002863131903682401, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.2408099896559348, 'clip_value': 5.519856935469093, 'weight_init': 'kaiming', 'patience': 11, 'batch_size': 64, 'num_epochs': 80}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:21:17,003] Trial 11 finished with value: 0.5697815418243408 and parameters: {'hidden_size': 256, 'learning_rate': 0.002159090754710586, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.27374547892766826, 'clip_value': 5.532774229835461, 'weight_init': 'normal', 'patience': 10, 'batch_size': 128, 'num_epochs': 118}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:22:15,276] Trial 12 finished with value: 0.5703883767127991 and parameters: {'hidden_size': 288, 'learning_rate': 0.0021617927363545546, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2526784341768953, 'clip_value': 5.964513885115901, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 132}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:23:10,925] Trial 13 finished with value: 0.5715085864067078 and parameters: {'hidden_size': 288, 'learning_rate': 0.0022272384125299137, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.20830591029825407, 'clip_value': 5.993746815284547, 'weight_init': 'kaiming', 'patience': 14, 'batch_size': 128, 'num_epochs': 133}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:24:32,580] Trial 14 finished with value: 0.5660474300384521 and parameters: {'hidden_size': 288, 'learning_rate': 0.0029419946986141846, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.19629009035992911, 'clip_value': 4.742762808467985, 'weight_init': 'kaiming', 'patience': 14, 'batch_size': 64, 'num_epochs': 131}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:25:23,493] Trial 15 finished with value: 0.5670742988586426 and parameters: {'hidden_size': 288, 'learning_rate': 0.0023024181290725487, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.19425588790297568, 'clip_value': 4.960085026202801, 'weight_init': 'kaiming', 'patience': 15, 'batch_size': 128, 'num_epochs': 156}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:26:17,035] Trial 16 finished with value: 0.5685213208198547 and parameters: {'hidden_size': 288, 'learning_rate': 0.002725763658159854, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.21450054871292779, 'clip_value': 4.867218179116241, 'weight_init': 'kaiming', 'patience': 8, 'batch_size': 128, 'num_epochs': 105}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:27:08,729] Trial 17 finished with value: 0.5603528618812561 and parameters: {'hidden_size': 288, 'learning_rate': 0.002308275543907607, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.29483806013737424, 'clip_value': 5.8491968815026185, 'weight_init': 'kaiming', 'patience': 12, 'batch_size': 128, 'num_epochs': 125}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:27:56,772] Trial 18 finished with value: 0.5335605144500732 and parameters: {'hidden_size': 288, 'learning_rate': 0.0020697905264498106, 'optimizer': 'adam', 'activation': 'tanh', 'dropout_rate': 0.21819449151989662, 'clip_value': 4.277309538948232, 'weight_init': 'kaiming', 'patience': 5, 'batch_size': 64, 'num_epochs': 141}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:29:20,149] Trial 19 finished with value: 0.5689880847930908 and parameters: {'hidden_size': 288, 'learning_rate': 0.0024057527230819404, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.301915431482175, 'clip_value': 5.1606527468707615, 'weight_init': 'xavier', 'patience': 18, 'batch_size': 128, 'num_epochs': 109}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:29:54,013] Trial 20 finished with value: 0.5640870332717896 and parameters: {'hidden_size': 288, 'learning_rate': 0.002805030619660115, 'optimizer': 'rmsprop', 'activation': 'relu', 'dropout_rate': 0.17882870020821962, 'clip_value': 3.4898985442697645, 'weight_init': 'kaiming', 'patience': 8, 'batch_size': 128, 'num_epochs': 125}. Best is trial 0 with value: 0.5719753503799438.\n",
      "[I 2025-12-07 20:31:06,252] Trial 21 finished with value: 0.5727221965789795 and parameters: {'hidden_size': 288, 'learning_rate': 0.002189727974068661, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2527558438694584, 'clip_value': 5.986843108101061, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 134}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:32:04,011] Trial 22 finished with value: 0.5649738907814026 and parameters: {'hidden_size': 288, 'learning_rate': 0.0022198241069277883, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.264223495342594, 'clip_value': 5.312817316898341, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 136}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:33:15,327] Trial 23 finished with value: 0.5702016353607178 and parameters: {'hidden_size': 288, 'learning_rate': 0.002049609627242749, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.22737471099534678, 'clip_value': 5.992468090725133, 'weight_init': 'kaiming', 'patience': 17, 'batch_size': 128, 'num_epochs': 148}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:34:12,433] Trial 24 finished with value: 0.5673543810844421 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019820848095228206, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.31725919930444735, 'clip_value': 4.575741797642275, 'weight_init': 'kaiming', 'patience': 14, 'batch_size': 128, 'num_epochs': 160}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:35:12,220] Trial 25 finished with value: 0.569501519203186 and parameters: {'hidden_size': 288, 'learning_rate': 0.002245521803362265, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2657783021020432, 'clip_value': 5.551434228515977, 'weight_init': 'kaiming', 'patience': 11, 'batch_size': 128, 'num_epochs': 125}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:36:21,360] Trial 26 finished with value: 0.5245519280433655 and parameters: {'hidden_size': 288, 'learning_rate': 0.002396959227028382, 'optimizer': 'adam', 'activation': 'tanh', 'dropout_rate': 0.23760916179521144, 'clip_value': 3.915005787031649, 'weight_init': 'kaiming', 'patience': 15, 'batch_size': 64, 'num_epochs': 137}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:37:24,487] Trial 27 finished with value: 0.5671676993370056 and parameters: {'hidden_size': 288, 'learning_rate': 0.0025594116560958536, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.28247288539146925, 'clip_value': 5.212019251728598, 'weight_init': 'orthogonal', 'patience': 12, 'batch_size': 128, 'num_epochs': 123}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:38:03,328] Trial 28 finished with value: 0.5602595210075378 and parameters: {'hidden_size': 288, 'learning_rate': 0.002178432736440221, 'optimizer': 'rmsprop', 'activation': 'relu', 'dropout_rate': 0.2021835666360726, 'clip_value': 4.5263492255177065, 'weight_init': 'xavier', 'patience': 9, 'batch_size': 128, 'num_epochs': 111}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:38:19,913] Trial 29 finished with value: 0.5672610402107239 and parameters: {'hidden_size': 256, 'learning_rate': 0.0023835879770411706, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.14707302326851945, 'clip_value': 3.6991046353076875, 'weight_init': 'orthogonal', 'patience': 6, 'batch_size': 128, 'num_epochs': 144}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:39:21,542] Trial 30 finished with value: 0.5643670558929443 and parameters: {'hidden_size': 288, 'learning_rate': 0.0023027032251391848, 'optimizer': 'rmsprop', 'activation': 'relu', 'dropout_rate': 0.3204193203432386, 'clip_value': 5.6958143275111945, 'weight_init': 'kaiming', 'patience': 14, 'batch_size': 128, 'num_epochs': 136}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:40:23,572] Trial 31 finished with value: 0.5676811337471008 and parameters: {'hidden_size': 288, 'learning_rate': 0.0021278185040974045, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2495778456009826, 'clip_value': 5.9198617510958345, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 131}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:41:18,750] Trial 32 finished with value: 0.5660474300384521 and parameters: {'hidden_size': 288, 'learning_rate': 0.0020281696806913117, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.25292331947714625, 'clip_value': 5.206311611424026, 'weight_init': 'kaiming', 'patience': 11, 'batch_size': 128, 'num_epochs': 149}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:42:06,455] Trial 33 finished with value: 0.5672143697738647 and parameters: {'hidden_size': 288, 'learning_rate': 0.002221796037048409, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.17243559494645738, 'clip_value': 3.033796599890684, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 129}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:42:57,625] Trial 34 finished with value: 0.5673543810844421 and parameters: {'hidden_size': 288, 'learning_rate': 0.001975116252377458, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.22844815771598823, 'clip_value': 5.992491760445703, 'weight_init': 'kaiming', 'patience': 15, 'batch_size': 128, 'num_epochs': 119}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:44:21,631] Trial 35 finished with value: 0.5395817756652832 and parameters: {'hidden_size': 288, 'learning_rate': 0.002455413023361164, 'optimizer': 'rmsprop', 'activation': 'tanh', 'dropout_rate': 0.2784892810898796, 'clip_value': 5.545403920484043, 'weight_init': 'orthogonal', 'patience': 16, 'batch_size': 128, 'num_epochs': 140}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:44:41,480] Trial 36 finished with value: 0.570715069770813 and parameters: {'hidden_size': 256, 'learning_rate': 0.0026454362239842107, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2509600202573853, 'clip_value': 4.137476616508161, 'weight_init': 'normal', 'patience': 12, 'batch_size': 128, 'num_epochs': 134}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:45:13,528] Trial 37 finished with value: 0.5334204435348511 and parameters: {'hidden_size': 256, 'learning_rate': 0.0026453423315850894, 'optimizer': 'adamw', 'activation': 'tanh', 'dropout_rate': 0.11779667212850875, 'clip_value': 4.27374755093979, 'weight_init': 'normal', 'patience': 9, 'batch_size': 64, 'num_epochs': 115}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:45:34,531] Trial 38 finished with value: 0.5660941004753113 and parameters: {'hidden_size': 256, 'learning_rate': 0.0026021062486894847, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.35265523169808843, 'clip_value': 2.3472257684405196, 'weight_init': 'normal', 'patience': 7, 'batch_size': 128, 'num_epochs': 146}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:45:47,634] Trial 39 finished with value: 0.5670742988586426 and parameters: {'hidden_size': 256, 'learning_rate': 0.0024992877922941897, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.15870776207280263, 'clip_value': 3.013050465803042, 'weight_init': 'normal', 'patience': 5, 'batch_size': 128, 'num_epochs': 152}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:46:22,168] Trial 40 finished with value: 0.5356142520904541 and parameters: {'hidden_size': 256, 'learning_rate': 0.002785858746405428, 'optimizer': 'rmsprop', 'activation': 'tanh', 'dropout_rate': 0.21097347194679456, 'clip_value': 3.7578131256912974, 'weight_init': 'normal', 'patience': 18, 'batch_size': 128, 'num_epochs': 121}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:46:46,618] Trial 41 finished with value: 0.5670276284217834 and parameters: {'hidden_size': 256, 'learning_rate': 0.0029997039633626182, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2622202381009136, 'clip_value': 4.119220516727618, 'weight_init': 'xavier', 'patience': 12, 'batch_size': 128, 'num_epochs': 128}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:47:15,168] Trial 42 finished with value: 0.5719286799430847 and parameters: {'hidden_size': 256, 'learning_rate': 0.0021337020971665117, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2459905146744595, 'clip_value': 4.977367343869721, 'weight_init': 'normal', 'patience': 14, 'batch_size': 128, 'num_epochs': 134}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:48:02,710] Trial 43 finished with value: 0.5698749423027039 and parameters: {'hidden_size': 256, 'learning_rate': 0.0026704674001799735, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.23666640927305813, 'clip_value': 4.514224301812573, 'weight_init': 'normal', 'patience': 14, 'batch_size': 128, 'num_epochs': 134}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:48:26,407] Trial 44 finished with value: 0.5678211450576782 and parameters: {'hidden_size': 256, 'learning_rate': 0.00212781127027451, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3971781371094356, 'clip_value': 5.0519552024824534, 'weight_init': 'normal', 'patience': 10, 'batch_size': 128, 'num_epochs': 140}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:49:34,095] Trial 45 finished with value: 0.5693147778511047 and parameters: {'hidden_size': 256, 'learning_rate': 0.002516499389970768, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.18595409305687344, 'clip_value': 4.782836210487228, 'weight_init': 'normal', 'patience': 16, 'batch_size': 64, 'num_epochs': 128}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:50:13,519] Trial 46 finished with value: 0.5691280961036682 and parameters: {'hidden_size': 256, 'learning_rate': 0.002253200187808277, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.29237063443221123, 'clip_value': 5.441832223074639, 'weight_init': 'normal', 'patience': 17, 'batch_size': 128, 'num_epochs': 116}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:50:40,389] Trial 47 finished with value: 0.5671209692955017 and parameters: {'hidden_size': 256, 'learning_rate': 0.0023379208407268257, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.24741475846241115, 'clip_value': 5.722643128043286, 'weight_init': 'normal', 'patience': 15, 'batch_size': 128, 'num_epochs': 142}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:51:00,730] Trial 48 finished with value: 0.5644604563713074 and parameters: {'hidden_size': 256, 'learning_rate': 0.0020002790291873403, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.31220210751653593, 'clip_value': 4.3910614630592555, 'weight_init': 'normal', 'patience': 12, 'batch_size': 128, 'num_epochs': 135}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:51:33,812] Trial 49 finished with value: 0.5638536214828491 and parameters: {'hidden_size': 256, 'learning_rate': 0.0019386787490671337, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.3322978767619724, 'clip_value': 4.025427171425708, 'weight_init': 'xavier', 'patience': 11, 'batch_size': 64, 'num_epochs': 122}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:52:15,039] Trial 50 finished with value: 0.5533514022827148 and parameters: {'hidden_size': 256, 'learning_rate': 0.002095610146510688, 'optimizer': 'adamw', 'activation': 'tanh', 'dropout_rate': 0.22849400117315988, 'clip_value': 4.787346274205827, 'weight_init': 'orthogonal', 'patience': 14, 'batch_size': 128, 'num_epochs': 133}. Best is trial 21 with value: 0.5727221965789795.\n",
      "[I 2025-12-07 20:53:16,778] Trial 51 finished with value: 0.5737023949623108 and parameters: {'hidden_size': 288, 'learning_rate': 0.0021812063853770126, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2842802372131165, 'clip_value': 5.738703375814433, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 138}. Best is trial 51 with value: 0.5737023949623108.\n",
      "[I 2025-12-07 20:54:27,933] Trial 52 finished with value: 0.572022020816803 and parameters: {'hidden_size': 288, 'learning_rate': 0.0021605843759550302, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2777730281243072, 'clip_value': 5.719931816043882, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 138}. Best is trial 51 with value: 0.5737023949623108.\n",
      "[I 2025-12-07 20:55:21,887] Trial 53 finished with value: 0.5639936327934265 and parameters: {'hidden_size': 288, 'learning_rate': 0.0021785786935405228, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.28078961111300865, 'clip_value': 5.734156195222571, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 138}. Best is trial 51 with value: 0.5737023949623108.\n",
      "[I 2025-12-07 20:56:20,717] Trial 54 finished with value: 0.5709951519966125 and parameters: {'hidden_size': 288, 'learning_rate': 0.0021171203274791763, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.26706982473657925, 'clip_value': 5.348262258546812, 'weight_init': 'kaiming', 'patience': 14, 'batch_size': 128, 'num_epochs': 145}. Best is trial 51 with value: 0.5737023949623108.\n",
      "[I 2025-12-07 20:57:29,026] Trial 55 finished with value: 0.566000759601593 and parameters: {'hidden_size': 288, 'learning_rate': 0.0022678591502716663, 'optimizer': 'adam', 'activation': 'relu', 'dropout_rate': 0.3028934176234127, 'clip_value': 5.730536030415041, 'weight_init': 'kaiming', 'patience': 15, 'batch_size': 128, 'num_epochs': 96}. Best is trial 51 with value: 0.5737023949623108.\n",
      "[I 2025-12-07 20:58:45,645] Trial 56 finished with value: 0.5719753503799438 and parameters: {'hidden_size': 288, 'learning_rate': 0.0022025030958093147, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.28798539979443677, 'clip_value': 5.04004748179975, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 130}. Best is trial 51 with value: 0.5737023949623108.\n",
      "[I 2025-12-07 21:00:05,993] Trial 57 finished with value: 0.5725354552268982 and parameters: {'hidden_size': 288, 'learning_rate': 0.00216674570554053, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2878076261500479, 'clip_value': 5.018231011729225, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 127}. Best is trial 51 with value: 0.5737023949623108.\n",
      "[I 2025-12-07 21:01:13,105] Trial 58 finished with value: 0.5681478977203369 and parameters: {'hidden_size': 288, 'learning_rate': 0.0021910948494241224, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.2931240157500061, 'clip_value': 5.064064570756109, 'weight_init': 'kaiming', 'patience': 11, 'batch_size': 128, 'num_epochs': 127}. Best is trial 51 with value: 0.5737023949623108.\n",
      "[I 2025-12-07 21:02:13,657] Trial 59 finished with value: 0.567307710647583 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018058423183296506, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3355317031279201, 'clip_value': 5.381518243844611, 'weight_init': 'kaiming', 'patience': 13, 'batch_size': 128, 'num_epochs': 130}. Best is trial 51 with value: 0.5737023949623108.\n",
      "[I 2025-12-07 21:04:18,267] Trial 60 finished with value: 0.5724887847900391 and parameters: {'hidden_size': 288, 'learning_rate': 0.0023427884560542037, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3067842434712028, 'clip_value': 4.659603848491906, 'weight_init': 'kaiming', 'patience': 12, 'batch_size': 64, 'num_epochs': 124}. Best is trial 51 with value: 0.5737023949623108.\n",
      "[I 2025-12-07 21:06:21,740] Trial 61 finished with value: 0.575102686882019 and parameters: {'hidden_size': 288, 'learning_rate': 0.002056835640777972, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.31184771476546386, 'clip_value': 4.716313243138867, 'weight_init': 'kaiming', 'patience': 12, 'batch_size': 64, 'num_epochs': 125}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:07:54,088] Trial 62 finished with value: 0.5734223127365112 and parameters: {'hidden_size': 288, 'learning_rate': 0.002062311300535819, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.30890571639185876, 'clip_value': 4.764937026993712, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 118}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:08:53,124] Trial 63 finished with value: 0.5661874413490295 and parameters: {'hidden_size': 288, 'learning_rate': 0.0020655285385863175, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.30943139801569663, 'clip_value': 4.667189240990245, 'weight_init': 'kaiming', 'patience': 10, 'batch_size': 64, 'num_epochs': 111}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:09:53,401] Trial 64 finished with value: 0.5658607482910156 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019285206964326596, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.320461096084908, 'clip_value': 5.617915091724818, 'weight_init': 'kaiming', 'patience': 12, 'batch_size': 64, 'num_epochs': 124}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:11:14,969] Trial 65 finished with value: 0.5750093460083008 and parameters: {'hidden_size': 288, 'learning_rate': 0.002029973437459457, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3658938994636557, 'clip_value': 5.3615130995372695, 'weight_init': 'kaiming', 'patience': 10, 'batch_size': 64, 'num_epochs': 121}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:12:53,115] Trial 66 finished with value: 0.5723954439163208 and parameters: {'hidden_size': 288, 'learning_rate': 0.0020057447532029213, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.36600206314169154, 'clip_value': 4.890817407945828, 'weight_init': 'kaiming', 'patience': 10, 'batch_size': 64, 'num_epochs': 119}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:14:08,027] Trial 67 finished with value: 0.5739358067512512 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018898348402519886, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.36951725771436916, 'clip_value': 0.852280552210845, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 117}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:15:26,619] Trial 68 finished with value: 0.574215829372406 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018819270287468452, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.36816008205407724, 'clip_value': 0.8945397270854355, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 112}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:16:20,541] Trial 69 finished with value: 0.5676811337471008 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018419826271384762, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.36527353441770716, 'clip_value': 0.9359429435991183, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 109}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:17:22,172] Trial 70 finished with value: 0.5376213788986206 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018470324188700235, 'optimizer': 'adamw', 'activation': 'tanh', 'dropout_rate': 0.36777051992185705, 'clip_value': 1.2068878634171927, 'weight_init': 'kaiming', 'patience': 8, 'batch_size': 64, 'num_epochs': 104}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:18:46,701] Trial 71 finished with value: 0.5728622078895569 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019030110221598619, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.38288474793287913, 'clip_value': 1.4860694530912169, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 112}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:19:51,959] Trial 72 finished with value: 0.5705283880233765 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019010608059141316, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3866445494452992, 'clip_value': 1.5565317612638305, 'weight_init': 'kaiming', 'patience': 7, 'batch_size': 64, 'num_epochs': 113}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:21:07,475] Trial 73 finished with value: 0.5748226642608643 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019420514034177662, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3783233238225464, 'clip_value': 1.0414766755186409, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 105}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:22:29,762] Trial 74 finished with value: 0.5741224884986877 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019531764948873395, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3818657629012199, 'clip_value': 1.041137638872504, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 107}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:23:21,765] Trial 75 finished with value: 0.5709018111228943 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019567374936600716, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.34734291894608704, 'clip_value': 0.97919247512801, 'weight_init': 'kaiming', 'patience': 8, 'batch_size': 64, 'num_epochs': 98}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:24:45,495] Trial 76 finished with value: 0.5735623836517334 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018509919158374312, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.37253132867855954, 'clip_value': 1.9123794804220795, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 100}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:25:53,608] Trial 77 finished with value: 0.5643670558929443 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018524827818867946, 'optimizer': 'rmsprop', 'activation': 'relu', 'dropout_rate': 0.3741668662092641, 'clip_value': 1.9895954428015832, 'weight_init': 'kaiming', 'patience': 8, 'batch_size': 64, 'num_epochs': 91}. Best is trial 61 with value: 0.575102686882019.\n",
      "[I 2025-12-07 21:27:37,630] Trial 78 finished with value: 0.5765029788017273 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018800118847900327, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3998466527993291, 'clip_value': 1.1518650819354024, 'weight_init': 'kaiming', 'patience': 10, 'batch_size': 64, 'num_epochs': 106}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:28:40,266] Trial 79 finished with value: 0.5653472542762756 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018789637163431287, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3942388921705436, 'clip_value': 1.0872511670190868, 'weight_init': 'kaiming', 'patience': 10, 'batch_size': 64, 'num_epochs': 107}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:30:14,661] Trial 80 finished with value: 0.5717886686325073 and parameters: {'hidden_size': 288, 'learning_rate': 0.0020286682893355633, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3873493529625533, 'clip_value': 0.8138041052840773, 'weight_init': 'xavier', 'patience': 10, 'batch_size': 64, 'num_epochs': 107}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:32:01,467] Trial 81 finished with value: 0.5727688670158386 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019265414426089837, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3786945855086963, 'clip_value': 1.2478993454285336, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 102}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:33:18,610] Trial 82 finished with value: 0.570294976234436 and parameters: {'hidden_size': 288, 'learning_rate': 0.001857410097725722, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3589955017652578, 'clip_value': 1.817359356358233, 'weight_init': 'kaiming', 'patience': 7, 'batch_size': 64, 'num_epochs': 115}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:34:56,414] Trial 83 finished with value: 0.572115421295166 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018231120236921969, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.39993233376268694, 'clip_value': 1.4118605190361555, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 103}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:36:04,369] Trial 84 finished with value: 0.5691747665405273 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018793877335392924, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.34229482264386896, 'clip_value': 1.6377007727815638, 'weight_init': 'kaiming', 'patience': 10, 'batch_size': 64, 'num_epochs': 100}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:37:40,245] Trial 85 finished with value: 0.5730956196784973 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019792938441936937, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3561934278720982, 'clip_value': 1.1131534627441113, 'weight_init': 'orthogonal', 'patience': 11, 'batch_size': 64, 'num_epochs': 106}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:38:42,208] Trial 86 finished with value: 0.5375280380249023 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019045874376425803, 'optimizer': 'adamw', 'activation': 'tanh', 'dropout_rate': 0.32782607384664886, 'clip_value': 1.0588496571973807, 'weight_init': 'kaiming', 'patience': 8, 'batch_size': 64, 'num_epochs': 92}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:39:27,790] Trial 87 finished with value: 0.5589525699615479 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019426800916764455, 'optimizer': 'rmsprop', 'activation': 'relu', 'dropout_rate': 0.3725364888871333, 'clip_value': 1.3177333144124956, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 101}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:40:54,731] Trial 88 finished with value: 0.5761762857437134 and parameters: {'hidden_size': 288, 'learning_rate': 0.001958598081742409, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3601431376056941, 'clip_value': 0.8335298718565273, 'weight_init': 'kaiming', 'patience': 10, 'batch_size': 64, 'num_epochs': 109}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:41:52,860] Trial 89 finished with value: 0.5717886686325073 and parameters: {'hidden_size': 288, 'learning_rate': 0.0020221544617515298, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3903115499708435, 'clip_value': 0.9244061919764955, 'weight_init': 'kaiming', 'patience': 11, 'batch_size': 64, 'num_epochs': 110}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:42:55,789] Trial 90 finished with value: 0.566000759601593 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019662712752276145, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3488998660828891, 'clip_value': 0.8621640523253363, 'weight_init': 'kaiming', 'patience': 10, 'batch_size': 64, 'num_epochs': 116}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:44:15,542] Trial 91 finished with value: 0.5751960277557373 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018763355008701175, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.37807945355215256, 'clip_value': 1.1573166731639672, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 109}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:45:18,229] Trial 92 finished with value: 0.5724887847900391 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019159662843513591, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.36225895747579573, 'clip_value': 1.2295340935198997, 'weight_init': 'kaiming', 'patience': 10, 'batch_size': 64, 'num_epochs': 108}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:46:23,402] Trial 93 finished with value: 0.5697815418243408 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019982279299989338, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.38039143558278604, 'clip_value': 1.0307494406020572, 'weight_init': 'kaiming', 'patience': 11, 'batch_size': 64, 'num_epochs': 113}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:47:30,904] Trial 94 finished with value: 0.5725354552268982 and parameters: {'hidden_size': 288, 'learning_rate': 0.0019518402565262467, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.34227311762144763, 'clip_value': 0.8320251100442917, 'weight_init': 'kaiming', 'patience': 9, 'batch_size': 64, 'num_epochs': 105}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:48:33,548] Trial 95 finished with value: 0.5753360986709595 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018768877721766829, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3549471762526982, 'clip_value': 1.1430373973260382, 'weight_init': 'kaiming', 'patience': 8, 'batch_size': 64, 'num_epochs': 120}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:50:05,032] Trial 96 finished with value: 0.5763162970542908 and parameters: {'hidden_size': 288, 'learning_rate': 0.001824817786608952, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3524184621018091, 'clip_value': 1.1428476778594985, 'weight_init': 'xavier', 'patience': 7, 'batch_size': 64, 'num_epochs': 121}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:50:55,733] Trial 97 finished with value: 0.5700149536132812 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018718600930091756, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.3506895786822098, 'clip_value': 1.140026863024622, 'weight_init': 'xavier', 'patience': 6, 'batch_size': 64, 'num_epochs': 121}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:51:58,397] Trial 98 finished with value: 0.5728155374526978 and parameters: {'hidden_size': 288, 'learning_rate': 0.001836347337282025, 'optimizer': 'adamw', 'activation': 'relu', 'dropout_rate': 0.35894591151270494, 'clip_value': 1.6076402253701736, 'weight_init': 'xavier', 'patience': 7, 'batch_size': 64, 'num_epochs': 114}. Best is trial 78 with value: 0.5765029788017273.\n",
      "[I 2025-12-07 21:52:53,879] Trial 99 finished with value: 0.5414488315582275 and parameters: {'hidden_size': 288, 'learning_rate': 0.0018257134804237382, 'optimizer': 'adamw', 'activation': 'tanh', 'dropout_rate': 0.37942427987985, 'clip_value': 1.7051707282262667, 'weight_init': 'xavier', 'patience': 8, 'batch_size': 64, 'num_epochs': 110}. Best is trial 78 with value: 0.5765029788017273.\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION COMPLETE\n",
      "================================================================================\n",
      "Number of finished trials: 100\n",
      "Best trial: 78\n",
      "Best validation accuracy: 57.6503%\n",
      "\n",
      "Best hyperparameters:\n",
      "  hidden_size: 288\n",
      "  learning_rate: 0.0018800118847900327\n",
      "  optimizer: adamw\n",
      "  activation: relu\n",
      "  dropout_rate: 0.3998466527993291\n",
      "  clip_value: 1.1518650819354024\n",
      "  weight_init: kaiming\n",
      "  patience: 10\n",
      "  batch_size: 64\n",
      "  num_epochs: 106\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RNN Next Character Prediction - Hyperparameter Optimization with Optuna\n",
    "========================================================================\n",
    "This script optimizes a Vanilla RNN for next character prediction using \n",
    "Bayesian optimization (Optuna) and then runs 20 independent experiments.\n",
    "\n",
    "Fixed Constraints:\n",
    "- Train/Val/Test split: 30%/10%/60%\n",
    "- Sequence length: 20\n",
    "- Vanilla RNN architecture\n",
    "- One-hot encoding (via embedding)\n",
    "\n",
    "Optimized Hyperparameters:\n",
    "- Hidden units, epochs, learning rate, optimizer, activation, dropout, \n",
    "  clipping, weight init, patience, batch size\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import copy\n",
    "import json\n",
    "from pathlib import Path\n",
    "import gc\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Limpiar caché GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"GPU cache limpiado\")\n",
    "torch.cuda.memory_summary()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. DATA PREPARATION (FIXED CONSTRAINTS)\n",
    "# ============================================================================\n",
    "\n",
    "# Load text from file\n",
    "file_path = \"./timemachine.txt\"\n",
    "print(f\"Loading text from {file_path}...\")\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "full_text = raw_text.lower()\n",
    "\n",
    "# Text Preprocessing: ASCII, no capitals, no punctuation, 26 letters + space + unk\n",
    "allowed_chars_set = set('abcdefghijklmnopqrstuvwxyz ')\n",
    "unk_token = '<unk>'\n",
    "\n",
    "cleaned_full_text = []\n",
    "for char in full_text:\n",
    "    if char in allowed_chars_set:\n",
    "        cleaned_full_text.append(char)\n",
    "    else:\n",
    "        cleaned_full_text.append(unk_token)\n",
    "full_text = \"\".join(cleaned_full_text)\n",
    "\n",
    "# FIXED: Split text into train, val, test (30%, 10%, 60%)\n",
    "total_length = len(full_text)\n",
    "train_split_idx = int(0.3 * total_length)\n",
    "val_split_idx = int(0.1 * total_length) + train_split_idx\n",
    "\n",
    "train_text = full_text[:train_split_idx]\n",
    "val_text = full_text[train_split_idx:val_split_idx]\n",
    "test_text = full_text[val_split_idx:]\n",
    "\n",
    "print(f\"Total text length: {total_length}\")\n",
    "print(f\"Train text length: {len(train_text)}\")\n",
    "print(f\"Validation text length: {len(val_text)}\")\n",
    "print(f\"Test text length: {len(test_text)}\")\n",
    "\n",
    "# Character mapping\n",
    "chars = sorted(list(set(full_text)))\n",
    "vocab_size = len(chars)\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {chars}\\n\")\n",
    "\n",
    "# FIXED: Hyperparameters that cannot be modified\n",
    "seq_length = 20\n",
    "future_steps = 1\n",
    "\n",
    "# Dataset preparation\n",
    "def create_sequences(text, seq_length, future_steps):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(text) - seq_length - future_steps):\n",
    "        seq = text[i:i+seq_length]\n",
    "        target = text[i+seq_length:i+seq_length+future_steps]\n",
    "        X.append([char2idx[ch] for ch in seq])\n",
    "        Y.append([char2idx[ch] for ch in target])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "print(\"Creating sequences...\")\n",
    "X_train_seq, Y_train_seq = create_sequences(train_text, seq_length, future_steps)\n",
    "X_val_seq, Y_val_seq = create_sequences(val_text, seq_length, future_steps)\n",
    "X_test_seq, Y_test_seq = create_sequences(test_text, seq_length, future_steps)\n",
    "\n",
    "X_train = torch.tensor(X_train_seq, dtype=torch.long).to(device)\n",
    "Y_train = torch.tensor(Y_train_seq, dtype=torch.long).to(device)\n",
    "X_val = torch.tensor(X_val_seq, dtype=torch.long).to(device)\n",
    "Y_val = torch.tensor(Y_val_seq, dtype=torch.long).to(device)\n",
    "X_test_final = torch.tensor(X_test_seq, dtype=torch.long).to(device)\n",
    "Y_test_final = torch.tensor(Y_test_seq, dtype=torch.long).to(device)\n",
    "\n",
    "print(f\"Training sequences: {X_train.shape}\")\n",
    "print(f\"Validation sequences: {X_val.shape}\")\n",
    "print(f\"Test sequences: {X_test_final.shape}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. VANILLA RNN MODEL (WITH OPTIMIZABLE COMPONENTS)\n",
    "# ============================================================================\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, future_steps, activation='relu', \n",
    "                 dropout_rate=0.0, weight_init='xavier'):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.future_steps = future_steps\n",
    "        self.activation = activation\n",
    "        self.weight_init = weight_init\n",
    "        \n",
    "        # FIXED: Embedding simulating one-hot encoding\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "        # RNN with configurable activation\n",
    "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True, \n",
    "                         nonlinearity=activation)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, future_steps * vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights based on selected method\"\"\"\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if self.weight_init == 'xavier':\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif self.weight_init == 'kaiming':\n",
    "                    nn.init.kaiming_uniform_(param, nonlinearity=self.activation)\n",
    "                elif self.weight_init == 'orthogonal':\n",
    "                    nn.init.orthogonal_(param)\n",
    "                elif self.weight_init == 'normal':\n",
    "                    nn.init.normal_(param, mean=0, std=0.01)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "        \n",
    "        # Initialize output layer\n",
    "        if self.weight_init == 'xavier':\n",
    "            nn.init.xavier_uniform_(self.fc.weight)\n",
    "        elif self.weight_init == 'kaiming':\n",
    "            nn.init.kaiming_uniform_(self.fc.weight, nonlinearity='linear')\n",
    "        elif self.weight_init == 'orthogonal':\n",
    "            nn.init.orthogonal_(self.fc.weight)\n",
    "        elif self.weight_init == 'normal':\n",
    "            nn.init.normal_(self.fc.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]  # last time step\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = out.view(-1, self.future_steps, vocab_size)\n",
    "        return out\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(hidden_size, num_epochs, learning_rate, optimizer_name, \n",
    "                activation, dropout_rate, clip_value, weight_init, patience,\n",
    "                batch_size, verbose=False):\n",
    "    \"\"\"\n",
    "    Train the RNN model with given hyperparameters.\n",
    "    Returns validation accuracy and test accuracy.\n",
    "    \"\"\"\n",
    "    # Create model\n",
    "    model = VanillaRNN(vocab_size, hidden_size, future_steps, \n",
    "                       activation=activation, dropout_rate=dropout_rate,\n",
    "                       weight_init=weight_init).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Select optimizer\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Create data loader for batching\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_Y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = sum(criterion(output[:, t, :], batch_Y[:, t]) for t in range(future_steps))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if clip_value > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val)\n",
    "            val_loss = sum(criterion(val_output[:, t, :], Y_val[:, t]) for t in range(future_steps))\n",
    "            \n",
    "            # Calculate validation accuracy\n",
    "            preds = val_output.argmax(dim=2)\n",
    "            val_acc = (preds == Y_val).float().mean().item()\n",
    "        \n",
    "        val_losses.append(val_loss.item())\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
    "        \n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "     # === EVALUACIÓN EN TEST CON DATALOADER ===\n",
    "    test_dataset = torch.utils.data.TensorDataset(X_test_final, Y_test_final)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=128,      # si quieres ir aún más seguro, usa 64\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in test_loader:\n",
    "            output = model(batch_X)\n",
    "            preds = output.argmax(dim=2)\n",
    "            correct += (preds == batch_Y).float().sum().item()\n",
    "            total += batch_Y.numel()\n",
    "\n",
    "    test_acc = correct / total\n",
    "    \n",
    "    return best_val_acc, test_acc, train_losses, val_losses, val_accuracies\n",
    "\n",
    "# ============================================================================\n",
    "# 4. OPTUNA HYPERPARAMETER OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function to maximize validation accuracy.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [256, 288])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.0018, 0.0030, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'adamw', 'rmsprop'])\n",
    "    activation = trial.suggest_categorical('activation', ['tanh', 'relu'])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.08, 0.4)\n",
    "    clip_value = trial.suggest_float('clip_value', 0.8, 6)\n",
    "    weight_init = trial.suggest_categorical('weight_init', ['xavier', 'kaiming', 'orthogonal', 'normal'])\n",
    "    patience = trial.suggest_int('patience', 5, 20)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 80, 160)\n",
    "    \n",
    "    # Train model\n",
    "    val_acc, test_acc, _, _, _ = train_model(\n",
    "        hidden_size=hidden_size,\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer_name=optimizer_name,\n",
    "        activation=activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        clip_value=clip_value,\n",
    "        weight_init=weight_init,\n",
    "        patience=patience,\n",
    "        batch_size=batch_size,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Report intermediate value for pruning\n",
    "    trial.report(val_acc, step=0)\n",
    "    \n",
    "    # Check if trial should be pruned\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    \n",
    "    return val_acc  # Maximize validation accuracy\n",
    "\n",
    "# Run Optuna optimization\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take a while depending on n_trials...\\n\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "\n",
    "# Run optimization (adjust n_trials based on computational resources)\n",
    "n_trials = 100\n",
    "print(f\"Running {n_trials} optimization trials...\")\n",
    "study.optimize(objective, n_trials=n_trials, timeout=None, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best trial: {study.best_trial.number}\")\n",
    "print(f\"Best validation accuracy: {study.best_value*100:.4f}%\")\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d183904c",
   "metadata": {},
   "source": [
    "## Entrenamiento Final\n",
    "Una vez que Optuna devuelve la mejor configuración de hiperparámetros para nuestro problema, se realiza un nuevo entrenamiento añadiendo un scheduler de la tasa de aprendizaje. Este mecanismo hace que el learning rate vaya disminuyendo progresivamente a lo largo de las épocas, lo que suaviza las actualizaciones de los gradientes en las fases finales del entrenamiento y favorece una convergencia más estable y, en muchos casos, una ligera mejora de la accuracy. En este caso, mejora ligeramente el accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f7ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_scheduler(hidden_size, num_epochs, learning_rate, optimizer_name, \n",
    "                activation, dropout_rate, clip_value, weight_init, patience,\n",
    "                batch_size, use_scheduler=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Train the RNN model with given hyperparameters.\n",
    "    Returns validation accuracy and test accuracy.\n",
    "    \"\"\"\n",
    "    # Create model\n",
    "    model = VanillaRNN(vocab_size, hidden_size, future_steps, \n",
    "                       activation=activation, dropout_rate=dropout_rate,\n",
    "                       weight_init=weight_init).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Select optimizer\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # ===== AÑADIDO: SCHEDULER =====\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=num_epochs, eta_min=learning_rate * 0.01\n",
    "        )\n",
    "    \n",
    "    # Training variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Create data loader for batching\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_Y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = sum(criterion(output[:, t, :], batch_Y[:, t]) for t in range(future_steps))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if clip_value > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # ===== AÑADIDO: STEP SCHEDULER =====\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(X_val)\n",
    "            val_loss = sum(criterion(val_output[:, t, :], Y_val[:, t]) for t in range(future_steps))\n",
    "            \n",
    "            # Calculate validation accuracy\n",
    "            preds = val_output.argmax(dim=2)\n",
    "            val_acc = (preds == Y_val).float().mean().item()\n",
    "        \n",
    "        val_losses.append(val_loss.item())\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%, LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # === EVALUACIÓN EN TEST CON DATALOADER ===\n",
    "    test_dataset = torch.utils.data.TensorDataset(X_test_final, Y_test_final)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in test_loader:\n",
    "            output = model(batch_X)\n",
    "            preds = output.argmax(dim=2)\n",
    "            correct += (preds == batch_Y).float().sum().item()\n",
    "            total += batch_Y.numel()\n",
    "\n",
    "    test_acc = correct / total\n",
    "    \n",
    "    return best_val_acc, test_acc, train_losses, val_losses, val_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22dcdcd",
   "metadata": {},
   "source": [
    "## Evaluación final con las 20 runs independientes\n",
    "Agrego 20 épocas más y 5 al patience, para maximizar la efectividad del scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0c76a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING 20 INDEPENDENT EXPERIMENTS WITH BEST HYPERPARAMETERS\n",
      "================================================================================\n",
      "\n",
      "Run 1/20...\n",
      "  Val Acc: 57.50%, Test Acc: 58.02%\n",
      "\n",
      "Run 2/20...\n",
      "  Val Acc: 57.87%, Test Acc: 58.03%\n",
      "\n",
      "Run 3/20...\n",
      "  Val Acc: 57.88%, Test Acc: 58.28%\n",
      "\n",
      "Run 4/20...\n",
      "  Val Acc: 57.48%, Test Acc: 57.88%\n",
      "\n",
      "Run 5/20...\n",
      "  Val Acc: 57.48%, Test Acc: 57.84%\n",
      "\n",
      "Run 6/20...\n",
      "  Val Acc: 57.44%, Test Acc: 57.66%\n",
      "\n",
      "Run 7/20...\n",
      "  Val Acc: 57.28%, Test Acc: 57.55%\n",
      "\n",
      "Run 8/20...\n",
      "  Val Acc: 56.58%, Test Acc: 56.87%\n",
      "\n",
      "Run 9/20...\n",
      "  Val Acc: 57.30%, Test Acc: 57.70%\n",
      "\n",
      "Run 10/20...\n",
      "  Val Acc: 57.09%, Test Acc: 57.47%\n",
      "\n",
      "Run 11/20...\n",
      "  Val Acc: 57.09%, Test Acc: 57.68%\n",
      "\n",
      "Run 12/20...\n",
      "  Val Acc: 57.16%, Test Acc: 57.47%\n",
      "\n",
      "Run 13/20...\n",
      "  Val Acc: 57.72%, Test Acc: 58.19%\n",
      "\n",
      "Run 14/20...\n",
      "  Val Acc: 57.49%, Test Acc: 57.83%\n",
      "\n",
      "Run 15/20...\n",
      "  Val Acc: 57.20%, Test Acc: 57.63%\n",
      "\n",
      "Run 16/20...\n",
      "  Val Acc: 57.91%, Test Acc: 58.17%\n",
      "\n",
      "Run 17/20...\n",
      "  Val Acc: 57.82%, Test Acc: 57.91%\n",
      "\n",
      "Run 18/20...\n",
      "  Val Acc: 57.03%, Test Acc: 57.48%\n",
      "\n",
      "Run 19/20...\n",
      "  Val Acc: 57.53%, Test Acc: 57.69%\n",
      "\n",
      "Run 20/20...\n",
      "  Val Acc: 57.19%, Test Acc: 57.37%\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS (20 independent runs)\n",
      "================================================================================\n",
      "Validation Accuracy: 57.4027% ± 0.3323%\n",
      "Test Accuracy: 57.7365% ± 0.3230%\n",
      "\n",
      "Best Test Accuracy: 58.2822%\n",
      "Worst Test Accuracy: 56.8675%\n",
      "Median Test Accuracy: 57.6972%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. FINAL EVALUATION WITH 20 INDEPENDENT RUNS\n",
    "# ============================================================================\n",
    "\n",
    "# Extract best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING 20 INDEPENDENT EXPERIMENTS WITH BEST HYPERPARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Storage for results\n",
    "final_test_accuracies = []\n",
    "final_val_accuracies = []\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "all_val_accuracies = []\n",
    "\n",
    "num_runs = 20\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"\\nRun {run+1}/{num_runs}...\")\n",
    "    \n",
    "    # Set different random seed for each run\n",
    "    torch.manual_seed(run)\n",
    "    np.random.seed(run)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(run)\n",
    "    \n",
    "    val_acc, test_acc, train_losses, val_losses, val_accuracies = train_model_scheduler(\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        num_epochs=best_params['num_epochs'] + 20,\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        optimizer_name=best_params['optimizer'],\n",
    "        activation=best_params['activation'],\n",
    "        dropout_rate=best_params['dropout_rate'],\n",
    "        clip_value=best_params['clip_value'],\n",
    "        weight_init=best_params['weight_init'],\n",
    "        patience=best_params['patience'] + 5,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    final_val_accuracies.append(val_acc)\n",
    "    final_test_accuracies.append(test_acc)\n",
    "    all_train_losses.append(train_losses)\n",
    "    all_val_losses.append(val_losses)\n",
    "    all_val_accuracies.append(val_accuracies)\n",
    "    \n",
    "    print(f\"  Val Acc: {val_acc*100:.2f}%, Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Calculate statistics\n",
    "mean_test_acc = np.mean(final_test_accuracies)\n",
    "std_test_acc = np.std(final_test_accuracies)\n",
    "mean_val_acc = np.mean(final_val_accuracies)\n",
    "std_val_acc = np.std(final_val_accuracies)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS (20 independent runs)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Validation Accuracy: {mean_val_acc*100:.4f}% ± {std_val_acc*100:.4f}%\")\n",
    "print(f\"Test Accuracy: {mean_test_acc*100:.4f}% ± {std_test_acc*100:.4f}%\")\n",
    "print(f\"\\nBest Test Accuracy: {max(final_test_accuracies)*100:.4f}%\")\n",
    "print(f\"Worst Test Accuracy: {min(final_test_accuracies)*100:.4f}%\")\n",
    "print(f\"Median Test Accuracy: {np.median(final_test_accuracies)*100:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e105927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Variables corregidas!\n",
      "mean_test_acc type: <class 'float'>\n",
      "mean_val_acc type: <class 'float'>\n",
      "\n",
      "Test Accuracy: 57.7365% ± 0.3230%\n",
      "Val Accuracy: 57.4027% ± 0.3323%\n",
      "\n",
      "Learning curves saved to 'learning_curves_final.png'\n",
      "Test accuracy distribution saved to 'test_accuracy_distribution.png'\n",
      "\n",
      "================================================================================\n",
      "FINAL EXPERIMENT SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "DATASET INFORMATION:\n",
      "- Total text length: 214451\n",
      "- Train/Val/Test split: 30%/10%/60% (FIXED)\n",
      "- Sequence length: 20 (FIXED)\n",
      "- Vocabulary size: 29\n",
      "- Future steps (prediction): 1\n",
      "\n",
      "BEST HYPERPARAMETERS (from Optuna optimization with 100 trials):\n",
      "- hidden_size: 288\n",
      "- learning_rate: 0.0018800118847900327\n",
      "- optimizer: adamw\n",
      "- activation: relu\n",
      "- dropout_rate: 0.3998466527993291\n",
      "- clip_value: 1.1518650819354024\n",
      "- weight_init: kaiming\n",
      "- patience: 10\n",
      "- batch_size: 64\n",
      "- num_epochs: 106\n",
      "\n",
      "FINAL RESULTS (20 independent runs with different initializations):\n",
      "- Mean Test Accuracy: 57.7365% ± 0.3230%\n",
      "- Mean Validation Accuracy: 57.4027% ± 0.3323%\n",
      "- Best Test Accuracy: 58.2822%\n",
      "- Worst Test Accuracy: 56.8675%\n",
      "- Median Test Accuracy: 57.6972%\n",
      "\n",
      "INDIVIDUAL RUN RESULTS:\n",
      "Run  1: Val Acc = 57.50%, Test Acc = 58.02%\n",
      "Run  2: Val Acc = 57.87%, Test Acc = 58.03%\n",
      "Run  3: Val Acc = 57.88%, Test Acc = 58.28%\n",
      "Run  4: Val Acc = 57.48%, Test Acc = 57.88%\n",
      "Run  5: Val Acc = 57.48%, Test Acc = 57.84%\n",
      "Run  6: Val Acc = 57.44%, Test Acc = 57.66%\n",
      "Run  7: Val Acc = 57.28%, Test Acc = 57.55%\n",
      "Run  8: Val Acc = 56.58%, Test Acc = 56.87%\n",
      "Run  9: Val Acc = 57.30%, Test Acc = 57.70%\n",
      "Run 10: Val Acc = 57.09%, Test Acc = 57.47%\n",
      "Run 11: Val Acc = 57.09%, Test Acc = 57.68%\n",
      "Run 12: Val Acc = 57.16%, Test Acc = 57.47%\n",
      "Run 13: Val Acc = 57.72%, Test Acc = 58.19%\n",
      "Run 14: Val Acc = 57.49%, Test Acc = 57.83%\n",
      "Run 15: Val Acc = 57.20%, Test Acc = 57.63%\n",
      "Run 16: Val Acc = 57.91%, Test Acc = 58.17%\n",
      "Run 17: Val Acc = 57.82%, Test Acc = 57.91%\n",
      "Run 18: Val Acc = 57.03%, Test Acc = 57.48%\n",
      "Run 19: Val Acc = 57.53%, Test Acc = 57.69%\n",
      "Run 20: Val Acc = 57.19%, Test Acc = 57.37%\n",
      "\n",
      "================================================================================\n",
      "END OF REPORT\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Report saved to 'final_experiment_report.txt'\n",
      "Best hyperparameters saved to 'best_hyperparameters.json'\n",
      "All results saved to 'all_results.json'\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 6. VISUALIZATION AND REPORT GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK FIX: Redefinir variables problemáticas\n",
    "# ============================================================================\n",
    "\n",
    "# Forzar conversión a escalares\n",
    "mean_test_acc = float(np.mean(final_test_accuracies))\n",
    "std_test_acc = float(np.std(final_test_accuracies))\n",
    "mean_val_acc = float(np.mean(final_val_accuracies))\n",
    "std_val_acc = float(np.std(final_val_accuracies))\n",
    "\n",
    "print(\"✅ Variables corregidas!\")\n",
    "print(f\"mean_test_acc type: {type(mean_test_acc)}\")\n",
    "print(f\"mean_val_acc type: {type(mean_val_acc)}\")\n",
    "print(f\"\\nTest Accuracy: {mean_test_acc*100:.4f}% ± {std_test_acc*100:.4f}%\")\n",
    "print(f\"Val Accuracy: {mean_val_acc*100:.4f}% ± {std_val_acc*100:.4f}%\")\n",
    "\n",
    "\n",
    "# Plot learning curves with standard deviation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Find maximum length for padding\n",
    "max_len_train = max(len(losses) for losses in all_train_losses)\n",
    "max_len_val = max(len(losses) for losses in all_val_losses)\n",
    "\n",
    "# Pad sequences with last value\n",
    "def pad_sequence(seq, max_len):\n",
    "    if len(seq) < max_len:\n",
    "        return seq + [seq[-1]] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "padded_train_losses = [pad_sequence(losses, max_len_train) for losses in all_train_losses]\n",
    "padded_val_losses = [pad_sequence(losses, max_len_val) for losses in all_val_losses]\n",
    "padded_val_accuracies = [pad_sequence(accs, max_len_val) for accs in all_val_accuracies]\n",
    "\n",
    "# Calculate mean and std\n",
    "mean_train_loss = np.mean(padded_train_losses, axis=0)\n",
    "std_train_loss = np.std(padded_train_losses, axis=0)\n",
    "mean_val_loss = np.mean(padded_val_losses, axis=0)\n",
    "std_val_loss = np.std(padded_val_losses, axis=0)\n",
    "mean_val_acc_curve = np.mean(padded_val_accuracies, axis=0)\n",
    "std_val_acc_curve = np.std(padded_val_accuracies, axis=0)\n",
    "\n",
    "epochs_train = range(1, max_len_train + 1)\n",
    "epochs_val = range(1, max_len_val + 1)\n",
    "\n",
    "# Plot Training Loss\n",
    "axes[0].plot(epochs_train, mean_train_loss, 'b-', linewidth=2, label='Mean')\n",
    "axes[0].fill_between(epochs_train, \n",
    "                     mean_train_loss - std_train_loss, \n",
    "                     mean_train_loss + std_train_loss,\n",
    "                     alpha=0.3, color='blue')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss (Mean ± Std)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot Validation Loss\n",
    "axes[1].plot(epochs_val, mean_val_loss, 'r-', linewidth=2, label='Mean')\n",
    "axes[1].fill_between(epochs_val, \n",
    "                     mean_val_loss - std_val_loss, \n",
    "                     mean_val_loss + std_val_loss,\n",
    "                     alpha=0.3, color='red')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_title('Validation Loss (Mean ± Std)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot Validation Accuracy\n",
    "axes[2].plot(epochs_val, [acc*100 for acc in mean_val_acc_curve], 'g-', linewidth=2, label='Mean')\n",
    "axes[2].fill_between(epochs_val, \n",
    "                     [(acc-std)*100 for acc, std in zip(mean_val_acc_curve, std_val_acc_curve)], \n",
    "                     [(acc+std)*100 for acc, std in zip(mean_val_acc_curve, std_val_acc_curve)],\n",
    "                     alpha=0.3, color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Validation Accuracy (%)')\n",
    "axes[2].set_title('Validation Accuracy (Mean ± Std)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curves_final.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nLearning curves saved to 'learning_curves_final.png'\")\n",
    "plt.close()\n",
    "\n",
    "# Plot distribution of test accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(np.array(final_test_accuracies)*100, bins=15, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(mean_test_acc*100, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {mean_test_acc*100:.2f}%')\n",
    "plt.xlabel('Test Accuracy (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Test Accuracies (20 runs)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('test_accuracy_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Test accuracy distribution saved to 'test_accuracy_distribution.png'\")\n",
    "plt.close()\n",
    "\n",
    "# Create comprehensive summary report\n",
    "summary_report = f\"\"\"\n",
    "{'='*80}\n",
    "FINAL EXPERIMENT SUMMARY REPORT\n",
    "{'='*80}\n",
    "\n",
    "DATASET INFORMATION:\n",
    "- Total text length: {total_length}\n",
    "- Train/Val/Test split: 30%/10%/60% (FIXED)\n",
    "- Sequence length: {seq_length} (FIXED)\n",
    "- Vocabulary size: {vocab_size}\n",
    "- Future steps (prediction): {future_steps}\n",
    "\n",
    "BEST HYPERPARAMETERS (from Optuna optimization with {n_trials} trials):\n",
    "\"\"\"\n",
    "\n",
    "for key, value in best_params.items():\n",
    "    summary_report += f\"- {key}: {value}\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "FINAL RESULTS (20 independent runs with different initializations):\n",
    "- Mean Test Accuracy: {mean_test_acc*100:.4f}% ± {std_test_acc*100:.4f}%\n",
    "- Mean Validation Accuracy: {mean_val_acc*100:.4f}% ± {std_val_acc*100:.4f}%\n",
    "- Best Test Accuracy: {max(final_test_accuracies)*100:.4f}%\n",
    "- Worst Test Accuracy: {min(final_test_accuracies)*100:.4f}%\n",
    "- Median Test Accuracy: {np.median(final_test_accuracies)*100:.4f}%\n",
    "\n",
    "INDIVIDUAL RUN RESULTS:\n",
    "\"\"\"\n",
    "\n",
    "for i, (val_acc, test_acc) in enumerate(zip(final_val_accuracies, final_test_accuracies), 1):\n",
    "    summary_report += f\"Run {i:2d}: Val Acc = {val_acc*100:.2f}%, Test Acc = {test_acc*100:.2f}%\\n\"\n",
    "\n",
    "summary_report += f\"\"\"\n",
    "{'='*80}\n",
    "END OF REPORT\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save report to file\n",
    "with open('final_experiment_report.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "print(\"\\nReport saved to 'final_experiment_report.txt'\")\n",
    "\n",
    "# Save best hyperparameters to file\n",
    "with open('best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_params, f, indent=4)\n",
    "print(\"Best hyperparameters saved to 'best_hyperparameters.json'\")\n",
    "\n",
    "# Save all results data\n",
    "results_data = {\n",
    "    'best_hyperparameters': best_params,\n",
    "    'test_accuracies': final_test_accuracies,\n",
    "    'val_accuracies': final_val_accuracies,\n",
    "    'statistics': {\n",
    "        'mean_test_acc': mean_test_acc,\n",
    "        'std_test_acc': std_test_acc,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'std_val_acc': std_val_acc,\n",
    "        'best_test_acc': max(final_test_accuracies),\n",
    "        'worst_test_acc': min(final_test_accuracies),\n",
    "        'median_test_acc': float(np.median(final_test_accuracies))\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('all_results.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=4)\n",
    "print(\"All results saved to 'all_results.json'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
